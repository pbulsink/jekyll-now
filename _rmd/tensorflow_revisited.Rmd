---
title: "TensorFlow Revisited"
author: "Phil Bulsink"
date: "2017-06-30"
output: html_document
status: process
published: true
layout: post
excerpt_separator: <!--more-->
maps: true
tags: R NLP RNN LSTM TensorFlow
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(R6)
library(tensorflow)
```

[Previously](https://pbulsink.github.io/blog/2017-06-21/tensorflow_intro.html), we visited TensorFlow and R and used it to (overfit) model a bit of language. Let's go back and work on a larger portion of text with a better model, to see what we can do to 'genericize' the predictions.

This time, we'll model our code on the tensorflow example discussed in their [tutorials](https://www.tensorflow.org/tutorials/recurrent), with the code hosted on their [GitHub](https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb). 

<!--more-->

As with them, we'll use the [Penn Tree Bank](https://catalog.ldc.upenn.edu/ldc99t42) dataset, particularly the files in the `data/` folder. As does the [`reader.py`](https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/reader.py) script, we'll convert each file's text to a numeric representation with a dictionary lookup. This will just be a formalized set of what we demonstrated last time, so look at the code for this post on [GitHub](https://github.com/pbulsink/pbulsink.github.io/tree/master/_rmd_published/rensorflow_revisited.Rmd) to see what's happening. 

```{r dataprep, include=FALSE, cache=TRUE}
integerize<-function(text, dictionary){
    text_v <- rep(0, length(text))
    for(i in 1:length(dictionary)){
        text_v[which(text == dictionary[[i]])]<-i
    }   
    return(text_v)
}

prepData<-function(datapath="./_data/"){
    
    ptb_test<-readLines(paste0(datapath, "ptb.test.txt"))
    ptb_train<-readLines(paste0(datapath, "ptb.train.txt"))
    ptb_valid<-readLines(paste0(datapath, "ptb.valid.txt"))
    
    ptb_test<-unlist(strsplit(paste(ptb_test, collapse="<eos>"), " "))
    ptb_train<-unlist(strsplit(paste(ptb_train, collapse="<eos>"), " "))
    ptb_valid<-unlist(strsplit(paste(ptb_valid, collapse="<eos>"), " "))
    
    vocab<-unique(ptb_test)
    
    test_v<-integerize(ptb_test, vocab)
    train_v<-integerize(ptb_train, vocab)
    valid_v<-integerize(ptb_valid, vocab)
    
    return(list(test_data=test_v, train_data=train_v, valid_data=valid_v, vocab=vocab))
}
```

We'll also set up a function to split data for feeding into our TensorFlow model, as the example does. This slices base data into pieces and returns those chunks for us.

```{r producer}
tensorProducer<-function(raw_data, batch_size, num_steps, name=NULL){
    batch_size<-as.integer(batch_size)
    num_steps<-as.integer(num_steps)
    raw_data<-as.integer(raw_data)
    sess<-tf$Session()
    
    with(tf$name_scope(name, "tensorProducer", list(raw_data, batch_size, num_steps)), {
        raw_data<-tf$convert_to_tensor(raw_data, dtype=tf$int32, name='raw_data')
        data_len<-tf$size(raw_data, out_type = tf$int32)
        batch_len<-data_len/batch_size
        batch_len<-tf$to_int32(batch_len)
        message(paste0(batch_len, ' = batch_len'))
        data<-tf$reshape(raw_data[0L:(sess$run(batch_len * batch_size)-1L)], shape(batch_size, sess$run(batch_len)))
        message(data)
        epoch_size<-as.integer((sess$run(batch_len)-1L) %/% num_steps)
        
        if(!epoch_size>0){
            stop("epoch_size == 0, decrease batch_size or num_steps")
        }
        
        epoch_size = tf$identity(epoch_size, name='epoch_size')
        i<-tf$train$range_input_producer(epoch_size, shuffle=FALSE)$dequeue()
        x<-tf$strided_slice(data, list(0L,(i*num_steps)), list(batch_size,((i+1L)*num_steps)))
        x$set_shape(shape(batch_size, num_steps))
        y<-tf$strided_slice(data, list(0L,(i*(num_steps+1L))), list(batch_size,((i+1L)*(num_steps+1L))))
        y$set_shape(shape(batch_size, num_steps))
        })
    return(list(x,y))
}
```

As does the example, we'll set up some auto model parameter sets to simplify operation. This is a volumnous bit of code, so again, check the github link for this post to see what's happening.
```{r model_parameters, include=FALSE}
paramChooser<-function(model_size = 'small'){
    if(model_size == 'large'){
        init_scale <- 0.04
        learning_rate <- 1.0
        max_grad_norm <- 10
        num_layers <- 2
        num_steps <- 35
        hidden_size <- 1500
        forget_bias<-0
        max_epoch <- 14
        max_max_epoch <- 55
        keep_prob <- 0.35
        lr_decay <- 1/1.15
        batch_size <- 20
        vocab_size <- 10000
    } else if (model_size == 'medium'){
        init_scale <- 0.05
        learning_rate <- 1.0
        max_grad_norm <- 5
        num_layers <- 2
        num_steps <- 35
        hidden_size <- 650
        forget_bias<-0
        max_epoch <- 6
        max_max_epoch <- 39
        keep_prob <- 0.5
        lr_decay <- 0.8
        batch_size <- 20
        vocab_size <- 10000
    } else if (model_size == 'test'){
        init_scale <- 0.1
        learning_rate <- 1.0
        max_grad_norm <- 1
        num_layers <- 1
        num_steps <- 2
        hidden_size <- 2
        forget_bias<-0
        max_epoch <- 1
        max_max_epoch <- 1
        keep_prob <- 1.0
        lr_decay <- 0.5
        batch_size <- 20
        vocab_size <- 10000
    } else { # default to small
        if (model_size != 'small'){
            message('Unknown model_size, defaulting to "small".')
        }
        init_scale <- 0.1
        learning_rate <- 1.0
        max_grad_norm <- 5
        num_layers <- 2
        num_steps <- 20
        hidden_size <- 200
        forget_bias<-0
        max_epoch <- 4
        max_max_epoch <- 13
        keep_prob <- 1.0
        lr_decay <- 0.5
        batch_size <- 20
        vocab_size <- 10000
    }
    return(list('init_scale'=init_scale, 'learning_rate'=learning_rate, 'max_grad_norm'=max_grad_norm, 
                'num_layers'=num_layers, 'num_steps'=num_steps, 'hidden_size'=hidden_size, 
                'forget_bias'=forget_bias, 'max_epoch'=max_epoch, 'max_max_epoch'=max_max_epoch, 
                'keep_prob'=keep_prob, 'lr_decay'=lr_decay, 'batch_size'=batch_size, 'vocab_size'=vocab_size))
}
```

Building the LSTM cell is **much** more involved for this model. I'll be using an [`R6`](https://cran.r-project.org/package=R6) style object to help mirror what's happening in the code. `R6` is one of a few ways of doing object oriented programming in R, and my favourite (although I haven't done a ton of work with any of them).

```{r model_object}
library(R6)

Model<-R6Class('Model', 
    public = list(
        batch_size = NULL,
        epoch_size=NULL,
        forget_bias = NULL,
        hidden_size = NULL,
        init_scale = NULL,
        input=NULL,
        is_training = NULL,
        keep_prob = NULL,
        learning_rate = NULL,
        lr_decay = NULL,
        max_epoch = NULL,
        max_grad_norm = NULL,
        max_max_epoch = NULL,
        num_layers = NULL,
        num_steps = NULL,
        outputs=NULL,
        output=NULL,
        params_type=NULL,
        vocab_size = NULL,
        
        initialize = function(is_training, params, inputs){
            private$unpack_params(params)
            self$params_type <- params_type
            self$is_training <- is_training
            self$input <- inputs
            cell<-private$make_cell()
            private$initial_state <- cell$zero_state(self$batch_size, tf$float32)
            embedding <- tf$get_variable('embedding', shape(self$vocab_size, self$hidden_size), dtype=tf$float32)
            private$x <- tf$nn$embedding_lookup(embedding, self$input$input_data)
            private$y <- self$input$targets
            if(self$is_training && self$keep_prob < 1){
                private$x <- tf$nn$dropout(private$x, self$keep_prob)
            }
            private$x <- tf$unstack(private$x, num=self$num_steps, axis=1L) 
            outputs <- tf$contrib$rnn$static_rnn(cell, private$x, private$initial_state)
            self$output <- tf$reshape(tf$stack(axis=1, values=outputs[[1]])[[self$n_input]])  # tf.reshape(tf.stack(axis=1, values=outputs), [-1, size])
    
            weights <- tf$get_variable('weights', shape(self$hidden_size, self$vocab_size), tf$float32)
            biases <- tf$get_variable('biases', shape(1, self$vocab_size), tf$float32)
            logits <- tf$matmul(self$output, weights) + biases
            
            loss<-tf$contrib$seq2seq$sequence_loss(logits, private$y, 
                                                   tf$ones(shape(batch_size, num_steps), 
                                                           average_across_timesteps=FALSE, average_across_batch=TRUE))
            private$cost<-tf$reduce_sum(loss)
            private$final_state<-outputs[[2]]
            
            if(self$is_training){
                private$lr = tf$Variable(0.0, trainable=FALSE)
                tvars = tf$trainable_variables()
                grads = tf$clip_by_global_norm(tf$gradients(private$cost, tvars),self$max_grad_norm)[[1]]
                optimizer = tf$train$GradientDescentOptimizer(private$lr)
                private$train_op = optimizer$apply_gradients(
                    Map(function(x,y) c(x,y), grads, tvars), #  zip(grads, tvars),
                    global_step=tf$contrib$framework$get_or_create_global_step())
            }
        },
        assign_lr=function(session, lr_value){
            new_lr<-tf$placeholder(tf$float32, shape(NULL), name="new_learning_rate")
            lr_update<-tf$assign(private$lr, new_lr)
            session$run(lr_update, feed_dict=dict(new_lr = lr_value))
        },
        get_lr=function(){
            return(private$lr)
        },
        get_train_op=function(){
            return(private$train_op)
        },
        get_final_state=function(){
            return(private$final_state)
        },
        get_inital_state=function(){
            return(private$initial_state)
        },
        get_cost=function(){
            return(private$cost)
        },
        get_input=function(){
            return(list(x=private$x, y=private$y))
        }
    ),
    private = list(
        lr=NULL,
        train_op=NULL,
        initial_state=NULL,
        final_state=NULL,
        cost=NULL,
        x=NULL,
        y=NULL,
        
        unpack_params=function(params){
            self$init_scale <- params$init_scale
            self$learning_rate <- params$learning_rate
            self$max_grad_norm <- params$max_grad_norm
            self$num_layers <- params$num_layers
            self$num_steps <- params$num_steps
            self$hidden_size <- params$hidden_size
            self$forget_bias <- params$forget_bias
            self$max_epoch <- params$max_epoch
            self$max_max_epoch <- params$max_max_epoch
            self$keep_prob <- params$keep_prob
            self$lr_decay <- params$lr_decay
            self$batch_size <- params$batch_size
            self$vocab_size <- params$vocab_size
        },
        
        make_cell=function(){
            LSTMCell<-function(){
                lstm<-tf$contrib$rnn$BasicLSTMCell(self$hidden_size, forget_bias=self$forget_bias, 
                                                   state_is_tuple=TRUE, reuse=tf$get_variable_scope()$reuse)
                return(lstm)
            }    
            ATTNCell<-LSTMCell
            if((self$is_training) && (self$keep_prob > 1)){
                ATTNCell<-function(params){
                    attn<-tf$contrib$rnn$DropoutWrapper(LSTMCell(), output_keep_prob=self$keep_prob)
                    return(attn)
                }
            }
            
            if(self$num_layers==1){
                cell<-tf$contrib$rnn$RNNCell(ATTNCell(), state_is_tuple=TRUE)
            } else if (self$num_layers == 2) {
                cell<-tf$contrib$rnn$MultiRNNCell(list(ATTNCell(), ATTNCell()), state_is_tuple=TRUE)
            } else if (self$num_layers == 3) {
                cell<-tf$contrib$rnn$MultiRNNCell(list(ATTNCell(), ATTNCell(), ATTNCell()), state_is_tuple=TRUE)
            } else {  # Max 4 layers
                cell<-tf$contrib$rnn$MultiRNNCell(list(ATTNCell(), ATTNCell(), ATTNCell(), ATTNCell()), state_is_tuple=TRUE)
            }
            return(cell)
        }
    )
)
```

Ok. What we did there was generalize all of the required functions and variable for the model to be self contained. Now, we can run the model by calling on some of the variables. For example, although `train_op` is a variable, it's a variable pointing to a tensorflow operation (`optimizer$apply_gradients(...)`). We'll use this in the next chunk below to run each epoch. 

```{r epochs}
run_epoch<-function(session, model, eval_op=NULL, verbose=FALSE){
  
  start_time <- Sys.time()
  costs <- 0.0
  iters <- 0

  fetches<-c(cost, final_state)
  
  if(!is.null(eval_op)){
      fetches<-c(fetches, eval_op)
  }
  
  for (step in c(1:model$input$epoch_size)){
    feed_dict <- dict()
    for (i in 1:length(model$get_initial_state)){
      feed_dict[[i]][c] <- state[i]$c
      feed_dict[[i]][h] <- state[i]$h
    }

    vals <- session$run(fetches=fetches, feed_dict=feed_dict)
    cost <- vals$cost
    state <- vals$final_state

    costs <- costs + cost
    iters <- iters + model$input$num_steps

    if (verbose && step %% (model$epoch_size %/% 10) == 10)
      message(paste0(round(step * 1.0 / model$input$epoch_size,3)," perplexity: ",
                     round(exp(costs / iters),3), " speed: ",
                     round(iters * model$input$batch_size / as.numeric(Sys.time() - start_time),3), " wps")) 
  }
  return(exp(costs / iters))
}
```

We'll create another class for tensor input, this contains the processed data (passed through `tensorProducer`) as well as a few of the parameters to pass along. 

```{r dataprep}
tensorInput<-R6Class('tensorInput',
    public=list(
    batch_size=NULL,
    num_steps=NULL,
    epoch_size=NULL,
    input_data=NULL,
    targets=NULL,
    
    initialize<-function(params, data, name=NULL){
        self$batch_size <- params$batch_size
        self$num_steps <- params$num_steps
        self$epoch_size <- ((length(data) %/% batch_size) - 1) %/% num_steps
        t<-tensorProducer(data, self$batch_size, self$num_steps, name=name) 
        self$input_data <- t[[1]] 
        self$targets <- t[[2]]
    }
    )                  
)
```


Finally, the code to tie it all together and run it.

```{r main, eval=FALSE}
datapath<-"./_data/"
verbose<-TRUE
savepath<-"./data/tfmodels/"
params_type<-'small'

main<-function(){  

    data<-prepData(datapath)
    test_data<-data$test_data
    train_data<-data$train_data
    valid_data<-data$valid_data
    vocab<-data$vocab

    params = paramChooser(params_type)
    eval_config = paramChooser(params_type)
    eval_config$batch_size = 1
    eval_config$num_steps = 1

    with(tf$Graph()$as_default(), {
        initializer = tf$random_uniform_initializer(-config$init_scale, config$init_scale)
    })

    with(tf$name_scope("Train"), {
        train_input <- tensorInput(params=params, data=train_data, name="TrainInput")
        with(tf.variable_scope("Model", reuse=NULL, initializer=initializer), {
            m = Model(is_training=TRUE, params=params, inputs=train_input)
        })
        tf$summary$scalar("Training Loss", m$cost)
        tf$summary$scalar("Learning Rate", m$lr)
    })

    with(tf$name_scope("Valid"), {
      valid_input = tensorInput(params=params, data=valid_data, name="ValidInput")
      with(tf$variable_scope("Model", reuse=TRUE, initializer=initializer), {
        mvalid = Model(is_training=FALSE, params=params, inputs=valid_input)
        })
      tf$summary$scalar("Validation Loss", mvalid$cost)
    })
      
    with(tf$name_scope("Test"), {
      test_input = tensorInput(params=eval_config, data=test_data, name="TestInput")
      with(tf$variable_scope("Model", reuse=TRUE, initializer=initializer), {
        mtest = Model(is_training=FALSE, params=eval_config, inputs=test_input)
        })
    })

    sv = tf$train$Supervisor(logdir=savepath)
    with(sv$managed_session() %as% session, {
      for(i in 1:params$max_max_epoch){
        lr_decay = params$lr_decay ** max(i + 1 - params$max_epoch, 0.0)
        m$assign_lr(session, params$learning_rate * lr_decay)

        message(paste0("Epoch: ", i + 1,
                       " Learning rate: ", round(session$run(m$lr), 3)))
        train_perplexity = run_epoch(session, m, eval_op=m$train_op, verbose=TRUE)
        message(paste0("Epoch: ", i + 1, 
                       " Train Perplexity: ", round(train_perplexity, 3)))
        valid_perplexity = run_epoch(session, mvalid)
        message(paste0("Epoch: ", i+1, " Valid Perplexity: ", round(valid_perplexity, 3)))
      }
      test_perplexity = run_epoch(session, mtest)
      print("Test Perplexity: %.3f" % test_perplexity)

      if (!is.null(savepath)){
        message(paste0("Saving model to ", savepath)
        sv$saver$save(session, savepath, global_step=sv$global_step)
      }
    })

```

I've checked this all, and it runs, but takes a few hours on my system. I get comparable results to the example in the original code:



For reference, here's my session_info

```{r sysinfo}
sessionInfo()
```
